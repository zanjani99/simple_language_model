{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kneser_ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file_path='NLP-01-2-HW1-Data/mask.txt'\n",
    "mask_gold_file_path='NLP-01-2-HW1-Data/mask_gold.txt'\n",
    "\n",
    "incomplete_file_path='NLP-01-2-HW1-Data/incomplete.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path, 'r') as file:\n",
    "        txt=file.read()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(txt):\n",
    "    '''Preprocess the text by removing any unwanted characters,\n",
    "     converting all the text to lowercase'''\n",
    "\n",
    "\n",
    "    # remove any unwanted characters\n",
    "    txt=txt.replace('\\u200c',' ')\n",
    "    txt=txt.replace('\\n',' ')\n",
    "\n",
    "    txt= re.sub(r'[^\\w\\s]','',txt)\n",
    "    # convert all the english text to lowercase\n",
    "    txt=txt.lower()\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2word(txt):\n",
    "    # split the text into individual words.\n",
    "    words= txt.split(' ')\n",
    "    # remove '_' in case it appears individually as a word\n",
    "    if \"_\" in words:\n",
    "        words.remove(\"_\")\n",
    "    # delete empty strings\n",
    "    if \"\" in words:\n",
    "        words.remove(\"\")\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2sentence(txt):\n",
    "    # split the text into individual sentence.\n",
    "    sentences = re.split(';|\\.|\\n|\\?|\\؟',txt)\n",
    "    # delete empty strings\n",
    "    if '' in sentences:\n",
    "        sentences.remove('')\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(sentence):\n",
    "    trigrams=Counter()\n",
    "\n",
    "    for i in range(2, len(sentence)):\n",
    "        trigram=tuple(sentence[i-2:i+1])\n",
    "        trigrams[trigram] += 1\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KNSmoother:\n",
    "    \n",
    "    def __init__(self, trigrams,discount=0.75):\n",
    "        self.trigram_counts=trigrams\n",
    "        self.bigrams=self.get_bigrams()\n",
    "        self.unigrams=self.get_unigrams()\n",
    "        self.total_count=sum(self.unigrams.values())\n",
    "        self.discount=discount # Kneser-Ney discount parameter\n",
    "        \n",
    "    def get_bigrams(self):\n",
    "        bigrams=Counter()\n",
    "        for trigram, count in self.trigram_counts.items():\n",
    "            bigram=trigram[0:2]\n",
    "            bigrams[bigram] += count\n",
    "        return bigrams\n",
    "    \n",
    "    def get_unigrams(self):\n",
    "        unigrams=Counter()\n",
    "        for trigram, count in self.trigram_counts.items():\n",
    "            unigram=trigram[0]\n",
    "            unigrams[unigram] += count\n",
    "        return unigrams\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def kn_smooth(self, trigram):\n",
    "        trigram_count=self.trigram_counts[trigram]\n",
    "        bigram=trigram[0:2]\n",
    "        unigram=trigram[0]\n",
    "        bigram_count=self.bigrams[bigram]\n",
    "        unigram_count=self.unigrams[unigram]\n",
    "        # handling out of vocab\n",
    "        if bigram_count==0:\n",
    "            total_grams=sum(self.trigram_counts.values()) \n",
    "            return self.discount/total_grams\n",
    "        \n",
    "        \n",
    "        # Compute D and lambda values\n",
    "        D=self.discount * (self.get_num_distinct_trigrams_starting_with_bigram(bigram) / bigram_count)\n",
    "        lamb=(D / unigram_count) * self.get_num_trigrams_starting_with_bigram(bigram)\n",
    "        \n",
    "        # Compute P_kn value\n",
    "        P_continuation=(self.discount / bigram_count) * self.get_num_distinct_continuations(bigram)\n",
    "        P_trigram=max(trigram_count - self.discount, 0) / bigram_count\n",
    "        P_kn=P_trigram + lamb * P_continuation\n",
    "        \n",
    "        return P_kn\n",
    "\n",
    "    \n",
    "    def get_num_distinct_trigrams_starting_with_bigram(self, bigram):\n",
    "        num_distinct_trigrams=0\n",
    "        for trigram in self.trigram_counts:\n",
    "            if trigram[0:2] == bigram:\n",
    "                num_distinct_trigrams += 1\n",
    "        return num_distinct_trigrams\n",
    "    \n",
    "    def get_num_trigrams_starting_with_bigram(self, bigram):\n",
    "        num_trigrams=0\n",
    "        for trigram in self.trigram_counts:\n",
    "            if trigram[0:2] == bigram:\n",
    "                num_trigrams += self.trigram_counts[trigram]\n",
    "        return num_trigrams\n",
    "    \n",
    "    def get_num_distinct_continuations(self, bigram):\n",
    "        continuations=set()\n",
    "        for trigram in self.trigram_counts:\n",
    "            if trigram[0:2] == bigram:\n",
    "                continuations.add(trigram[2])\n",
    "        return len(continuations)\n",
    "    \n",
    "    def get_kn_smooth(self, sentence):\n",
    "        probs=[]\n",
    "        for i in range(2, len(sentence)):\n",
    "            trigram=tuple(sentence[i-2:i+1])\n",
    "            P_kn=self.kn_smooth(trigram)\n",
    "            probs.append(P_kn)\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def score_sentence(self, sentence):\n",
    "        score=0.0\n",
    "        for i in range(2, len(sentence)):\n",
    "            trigram=tuple(sentence[i-2:i+1])\n",
    "            P_kn=self.kn_smooth(trigram)\n",
    "            # print(math.log(P_kn, 2))\n",
    "            score += math.log(P_kn, 2)\n",
    "        return score\n",
    "    \n",
    "    def perplexity(self, corpus):\n",
    "        logprob=0\n",
    "        word_count=0\n",
    "        for sentence in corpus:\n",
    "            logprob += self.score_sentence(sentence)\n",
    "            word_count += len(sentence) - 2\n",
    "        logprob /= word_count\n",
    "        perplexity=2**(-logprob)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess train data\n",
    "train_txt=load_dataset(\"NLP-01-2-HW1-Data/train.txt\")\n",
    "train_txt=preprocess_txt(train_txt)\n",
    "train_words=txt2word(train_txt)\n",
    "trigram_words=generate_trigrams(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_courpus=load_dataset(\"NLP-01-2-HW1-Data/valid.txt\")\n",
    "preprocess_valid_courpus=[preprocess_txt(sentence) for sentence in txt2sentence(valid_courpus)]\n",
    "valid_words=[txt2word(sentence) for sentence in preprocess_valid_courpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resualts=[]\n",
    "# for  d in  [0.5, 0.7, 0.9]:\n",
    "#     kn_smoother=KNSmoother(trigram_words)\n",
    "\n",
    "#     perplexity=kn_smoother.perplexity(valid_words[:])\n",
    "#     print(f\"discount:{d},Perplexity: {perplexity}\")\n",
    "#     resualts.append((d,perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sentence=[]\n",
    "with open(mask_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "       \n",
    "        words=(re.split('\\u200c| ',(line.split('\\t')[1])))\n",
    "        for j in range(words.count('#')):\n",
    "            if j==0:\n",
    "                idx=0\n",
    "                pre_idx=0\n",
    "            else:\n",
    "                pre_idx=idx+1\n",
    "            idx=words.index('#',idx+1)\n",
    "            # print(words[pre_idx:idx],j)\n",
    "            mask_sentence.append(' '.join(words[pre_idx:idx]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_data=[]\n",
    "with open(incomplete_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "        words=(re.split('\\u200c| ',preprocess_txt(line.split('\\t')[1])))\n",
    "        incomplete_data.append(' '.join(words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(set(train_words))\n",
    "# vocab.remove('')\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_courpus=load_dataset(\"NLP-01-2-HW1-Data/test.txt\")\n",
    "preprocess_test_courpus=[preprocess_txt(sentence) for sentence in txt2sentence(test_courpus)]\n",
    "test_words=[txt2word(sentence) for sentence in preprocess_test_courpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_smoother=KNSmoother(trigram_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9849619284302127e-06"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_smoother.kn_smooth('پیشرفت را به')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kn_smoother.get_kn_smooth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability: -456.34306508166765\n"
     ]
    }
   ],
   "source": [
    "logprob=kn_smoother.score_sentence(test_words[0])\n",
    "print(f\"Log probability: {logprob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb Cell 23\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m perplexity\u001b[39m=\u001b[39mkn_smoother\u001b[39m.\u001b[39;49mperplexity(test_words)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerplexity: \u001b[39m\u001b[39m{\u001b[39;00mperplexity\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb Cell 23\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m word_count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m corpus:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     logprob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore_sentence(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     word_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentence) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m logprob \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m word_count\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb Cell 23\u001b[0m in \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39mlen\u001b[39m(sentence)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     trigram\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(sentence[i\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     P_kn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkn_smooth(trigram)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39m# print(math.log(P_kn, 2))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mlog(P_kn, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb Cell 23\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m lamb\u001b[39m=\u001b[39m(D \u001b[39m/\u001b[39m unigram_count) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_num_trigrams_starting_with_bigram(bigram)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Compute P_kn value\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m P_continuation\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount \u001b[39m/\u001b[39m bigram_count) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_distinct_continuations(bigram)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m P_trigram\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(trigram_count \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount, \u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m bigram_count\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m P_kn\u001b[39m=\u001b[39mP_trigram \u001b[39m+\u001b[39m lamb \u001b[39m*\u001b[39m P_continuation\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb Cell 23\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m continuations\u001b[39m=\u001b[39m\u001b[39mset\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m trigram \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrigram_counts:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mif\u001b[39;00m trigram[\u001b[39m0\u001b[39;49m:\u001b[39m2\u001b[39;49m] \u001b[39m==\u001b[39m bigram:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         continuations\u001b[39m.\u001b[39madd(trigram[\u001b[39m2\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW2.ipynb#X31sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(continuations)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perplexity=kn_smoother.perplexity(test_words)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence,kn_smoother,vocab):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        # print('small')\n",
    "        return None\n",
    "    elif len(words) == 2:\n",
    "        bi = (words[0], words[1])\n",
    "        tri_probs = [kn_smoother.kn_smooth(' '.join([bi[0], bi[1], word])) for word in vocab]\n",
    "        pred= vocab[tri_probs.index(max(tri_probs))]\n",
    "    else:\n",
    "        tri = (words[-3], words[-2], words[-1])\n",
    "        bi = (words[-2], words[-1])\n",
    "        tri_probs = [kn_smoother.kn_smooth(' '.join([tri[0], tri[1], word])) for word in vocab]\n",
    "        bi_probs = [kn_smoother.kn_smooth(' '.join([bi[0], bi[1], word])) for word in vocab]\n",
    "        probs = [p_tri * p_bi for p_tri, p_bi in zip(tri_probs, bi_probs)]\n",
    "        pred= vocab[probs.index(max(probs))]\n",
    "    # if pred=='':\n",
    "    #     idx=np.array(probs).argsort()[-2]\n",
    "    #     pred=vocab[idx]\n",
    "        # print(max(probs))\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sentence.split()\n",
    "bi = (words[0], words[1])\n",
    "[kn_smoother.kn_smooth(' '.join([bi[0], bi[1], word])) for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'عمومی ترین'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "بر دشمنان خود \n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "predicted_word = predict_next_word(mask_sentence[5],kn_smoother,vocab)\n",
    "if predicted_word=='':\n",
    "    print('predicted_word:','Null')\n",
    "else:\n",
    "    print('predicted_word:',predicted_word)\n",
    "print(' '.join([mask_sentence[5],predicted_word]))\n",
    "print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "عمومی ترین \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "جانوران که طی مراحل \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "نیز پیش از همه \n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence[6:]:\n",
    "    # print(sentence)\n",
    "    predicted_word = predict_next_word(sentence,kn_smoother,vocab)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "سارتر به عنوان روشن فکری فعال از نظر \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "این تیم در سال ۱۸۹۹ \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "شد و تا به حال موفق به کسب یک عنوان \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "بزرگترین کلیسای \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "کریم خان پس از \n",
      "_______________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb Cell 24\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m mask_sentence:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     predicted_word \u001b[39m=\u001b[39m predict_next_word(sentence,kn_smoother,vocab)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m predicted_word\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpredicted_word:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mNull\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m bi \u001b[39m=\u001b[39m (words[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], words[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m tri_probs \u001b[39m=\u001b[39m [kn_smoother\u001b[39m.\u001b[39mkn_smooth(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([tri[\u001b[39m0\u001b[39m], tri[\u001b[39m1\u001b[39m], word])) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocab]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m bi_probs \u001b[39m=\u001b[39m [kn_smoother\u001b[39m.\u001b[39mkn_smooth(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([bi[\u001b[39m0\u001b[39m], bi[\u001b[39m1\u001b[39m], word])) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocab]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m probs \u001b[39m=\u001b[39m [p_tri \u001b[39m*\u001b[39m p_bi \u001b[39mfor\u001b[39;00m p_tri, p_bi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tri_probs, bi_probs)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pred\u001b[39m=\u001b[39m vocab[probs\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(probs))]\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m bi \u001b[39m=\u001b[39m (words[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], words[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m tri_probs \u001b[39m=\u001b[39m [kn_smoother\u001b[39m.\u001b[39mkn_smooth(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([tri[\u001b[39m0\u001b[39m], tri[\u001b[39m1\u001b[39m], word])) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocab]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m bi_probs \u001b[39m=\u001b[39m [kn_smoother\u001b[39m.\u001b[39;49mkn_smooth(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin([bi[\u001b[39m0\u001b[39;49m], bi[\u001b[39m1\u001b[39;49m], word])) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocab]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m probs \u001b[39m=\u001b[39m [p_tri \u001b[39m*\u001b[39m p_bi \u001b[39mfor\u001b[39;00m p_tri, p_bi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tri_probs, bi_probs)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pred\u001b[39m=\u001b[39m vocab[probs\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(probs))]\n",
      "\u001b[1;32m/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb Cell 24\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# handling out of vocab\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m bigram_count\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     total_grams\u001b[39m=\u001b[39m\u001b[39msum\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrigram_counts\u001b[39m.\u001b[39;49mvalues()) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount\u001b[39m/\u001b[39mtotal_grams\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zoha/Documents/HW/nlp_hw1v2/NLP_HW1_2.ipynb#X44sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Compute D and lambda values\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence,kn_smoother,vocab)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "در جریان انقلاب مشروطه ابتدا به عنوان یکی از نیروهای محمدعلی شاه با   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "شرکت خدمات مالی و  \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "شخص موسی تورات را   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "نام امازون را از یک لغت نامه   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "تیم سپاهان اصفهان که در ابتدا شعبه  \n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,kn_smoother,vocab)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence='ساخت این شهر به این علت بود که می‌خواستند محل پایتخت برزیل که پیش از آن شهر ریو دوژانیرو بود را تغییر داده و بیشتر به مرکز و غرب کشور بیاورند تا پیشرفت را به مناطق داخلی‌تر برزیل توسعه دهد.'\n",
    "# sentence=preprocess_txt(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=txt2word(sentence)\n",
    "# kn_smoother=KNSmoother(trigram_words)\n",
    "# print(kn_smoother.get_kn_smooth(words))\n",
    "# logprob=kn_smoother.score_sentence(words)\n",
    "# perplexity=kn_smoother.perplexity([words])\n",
    "# print(f\"Log probability: {logprob}\")\n",
    "# print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_txt='the quick brown fox jumps over the lazy dog the lazy dog is not very quick the quick brown fox is very quick indeed'\n",
    "# words=txt2word(train_txt)\n",
    "# trigram_words=generate_trigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kn_smoother=KNSmoother(trigram_words)\n",
    "# sentence='the quick brown fox jumps'\n",
    "# words=txt2word(sentence)\n",
    "# kn_smoother.get_kn_smooth(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate the language model on a test sentence\n",
    "# test_sentence=['the quick brown fox jumps over the lazy dog']\n",
    "# test_sentence=txt2word(sentence)\n",
    "# logprob=kn_smoother.score_sentence(test_sentence)\n",
    "# perplexity=kn_smoother.perplexity([test_sentence])\n",
    "# print(f\"Log probability: {logprob}\")\n",
    "# print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
