{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_path='NLP-01-2-HW1-Data/valid.txt'\n",
    "test_file_path='NLP-01-2-HW1-Data/test.txt'\n",
    "\n",
    "mask_file_path='NLP-01-2-HW1-Data/mask.txt'\n",
    "mask_gold_file_path='NLP-01-2-HW1-Data/mask_gold.txt'\n",
    "\n",
    "incomplete_file_path='NLP-01-2-HW1-Data/incomplete.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read  and preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    # load the dataset and return the txt as string\n",
    "    with open(path, 'r') as file:\n",
    "        txt=file.read()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(txt):\n",
    "    '''Preprocess the text by removing any unwanted characters,\n",
    "     converting all the text to lowercase'''\n",
    "\n",
    "\n",
    "    # remove any unwanted characters\n",
    "    txt=txt.replace('\\u200c',' ')\n",
    "    txt=txt.replace('\\n',' ')\n",
    "\n",
    "    txt= re.sub(r'[^\\w\\s]','',txt)\n",
    "    # convert all the english text to lowercase\n",
    "    txt=txt.lower()\n",
    "    \n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2word(txt):\n",
    "    # split the text into individual words.\n",
    "    words= txt.split(' ')\n",
    "    # remove '_' in case it appears individually as a word\n",
    "    if \"_\" in words:\n",
    "        words.remove(\"_\")\n",
    "    # delete empty strings\n",
    "    if \"\" in words:\n",
    "        words.remove(\"\")\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2sentence(txt):\n",
    "    # split the text into individual sentence based on punctuation\n",
    "    sentences = re.split(';|\\.|\\n|\\?|\\ØŸ',txt)\n",
    "    # delete empty strings\n",
    "    if '' in sentences:\n",
    "        sentences.remove('')\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ø³Ù„Ø§Ù…', ' Ø¨Ú†Ù‡', 'Ø­Ø§Ù„Øª Ø®ÙˆØ¨Ù‡', 'Ú†Ù‡ Ø®Ø¨Ø± ', 'Ø³Ø§Ù„Ù…ÛŒ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2sentence(' Ø³Ù„Ø§Ù…? Ø¨Ú†Ù‡.Ø­Ø§Ù„Øª Ø®ÙˆØ¨Ù‡;Ú†Ù‡ Ø®Ø¨Ø± ØŸØ³Ø§Ù„Ù…ÛŒ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N grams functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(words,ngram=2):\n",
    "    # get list of words and create list of ngrams.\n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The above',\n",
       " 'above function',\n",
       " 'function inputs',\n",
       " 'inputs two',\n",
       " 'two parameters']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=txt2word('The above function inputs two parameters')\n",
    "generate_N_grams(words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laplce smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Ngram_counts(words):\n",
    "    # Creating N_gram dictionary {word: count}\n",
    "    Ngram_counts={}\n",
    "    for word in words:\n",
    "        if word not in Ngram_counts:\n",
    "            Ngram_counts[word]=0\n",
    "        Ngram_counts[word] += 1\n",
    "        \n",
    "    return Ngram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing_counts(Ngram_counts,V,k=1):\n",
    "    # Normalizing counts in Ngram dictionary to get probabilities and add lablce smoothing\n",
    "    '''k is smoothing parameter \n",
    "    V is the vocabulary size\n",
    "    (when k==1 it's laplace smoothing \n",
    "    otherwise it's add-k smoothing)'''\n",
    "     \n",
    "    smooth_Ngram=Ngram_counts.copy()\n",
    "    total_grams=sum(Ngram_counts.values()) # \n",
    "    for word,value in Ngram_counts.items():\n",
    "        if word not in Ngram_counts:\n",
    "            smooth_Ngram[word]=k/(total_grams+(k*V))\n",
    "        else:\n",
    "            smooth_Ngram[word]=(value+k)/(total_grams+(k*V))\n",
    "    return smooth_Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(trigram,tri_counts,V,k=1):\n",
    "    \"\"\"\n",
    "    calculate add one smoothing for single Ngram\n",
    "    if Ngram exist in dictionary --> retrun normalized count\n",
    "    if Ngram does not exist --> return constant(k/ncount of all n-grams in courpus+(k*V)) \n",
    "    trigram: string of N words\n",
    "        tri_count: N_gram dictionary\n",
    "        V: length of vocab\n",
    "    \"\"\"\n",
    "    total_grams=sum(tri_counts.values())\n",
    "    if trigram  not in tri_counts:\n",
    "        P_laplace=k/(total_grams+(k*V))\n",
    "    else:\n",
    "        P_laplace=(tri_counts[trigram]+k)/(total_grams+(k*V))\n",
    "    return P_laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence,n_counts,V,n):\n",
    "    # calculate Laplace smoothing and score of sentence\n",
    "    score=0.0\n",
    "    \n",
    "    for i in range(n-1, len(sentence)):\n",
    "        ngram=' '.join(sentence[i-(n-1):i+1])\n",
    "        \n",
    "        p_laplace=laplace_smoothing(ngram,n_counts,V)\n",
    "        \n",
    "\n",
    "        score += math.log( p_laplace, 2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(corpus,n_counts,vocab_size,n):\n",
    "    # calculate the perplexity using score of each sentence\n",
    "    logprob=0\n",
    "    word_count=0\n",
    "    for sentence in corpus:\n",
    "        logprob += score_sentence(sentence,n_counts,vocab_size,n)\n",
    "        word_count += len(sentence) - (n-1)\n",
    "    logprob /= word_count\n",
    "    perplexity=2**(-logprob)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# laod and process train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess train data\n",
    "train_txt=load_dataset(\"NLP-01-2-HW1-Data/train.txt\")\n",
    "train_txt=preprocess_txt(train_txt)\n",
    "train_words=txt2word(train_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_courpus=load_dataset(\"NLP-01-2-HW1-Data/test.txt\")\n",
    "preprocess_test_courpus=[preprocess_txt(sentence) for sentence in txt2sentence(test_courpus)]\n",
    "test_words=[txt2word(sentence) for sentence in preprocess_test_courpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load masked and incomplete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sentence=[]\n",
    "with open(mask_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "       \n",
    "        words=(re.split('\\u200c| ',(line.split('\\t')[1])))\n",
    "        for j in range(words.count('#')):\n",
    "            if j==0:\n",
    "                idx=0\n",
    "                pre_idx=0\n",
    "            else:\n",
    "                pre_idx=idx+1\n",
    "            idx=words.index('#',idx+1)\n",
    "            # print(words[pre_idx:idx],j)\n",
    "            mask_sentence.append(' '.join(words[pre_idx:idx]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_data=[]\n",
    "with open(incomplete_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "        words=(re.split('\\u200c| ',preprocess_txt(line.split('\\t')[1])))\n",
    "        incomplete_data.append(' '.join(words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(set(train_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print charchters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0123456789_abcdefghijklmnopqrstuvwxyzÃ¦Ã°Ä±É‘É’É”É™É›É¨ÉªÉ¾ÊÊƒÊŠÊËˆËÎ±Î²Î³Î´ÎµÎ·Î¸Î¹ÎºÎ»Î¼Î½Î¿ÏÏ‚ÏƒÏ„Ï…Ï‰Ğ°Ğ±Ğ³Ğ´ĞµĞ¸ĞºĞ»Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ‡ÑˆÑŠÑÑ–Õ¡Õ¢Õ¥Õ§Õ©Õ«Õ¬Õ­Õ¯Õ°Õ³Õ´ÕµÕ¶Õ¸Õ¹ÕºÕ½Õ¿Ö€Ö‚Ö„Ö…××‘×“×”×•×—×™×›×œ××Ÿ× ×¢×¨×©×ªØ¡Ø¢Ø£Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙ€ÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙ¡Ù¢Ù¥Ù§Ù¨Ù©Ù±Ù¾Ú†Ú˜Ú©Ú¯ÚµÛÛ†ÛŒÛÛÛ•Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹à¤ˆà¤à¤–à¤—à¤¦à¤¬à¤®à¤¯à¤°à¤²à¤¶à¤¸à¦•à¦ à¦¥à¦¦à¦¨à¦¬à¦°à®šà®©á„€á„ƒá„‹á„Œá„á„’á…¡á…¢á…§á…©á…µá†«á†¼â´°â´½âµâµ“âµ”âµ›ä¸‰ä¸­ä¸¹äº§ä½›å…šå…±å‹åŒ—å‰åŒå”å›½åœ‹åœŸå­å­—å­¦å®˜å®¶æŒ¯æ•æ–‡æ–¹æ™®æœææŸ´æ¦´æ±‰æ¼¢çŠ¬ç‹®çŒªç…çµç¶ç”¢ç”°çŸ³ç§€ç¾½èƒ¡è‡£èˆ¹èŠ¸è©è„è¨è‘¡è–©è—©èœœè¡“è©±èªè¯è¯­è±Šè±¬é€šéƒé…ªé•·é™¢é¦¬é©¬éµé¹…é»¨ğ¬€ğ¬ğ¬™ğ¬šğ¬­ğ¬°ğ¬±ğ­¥ğ­§ğ­©ğ­¯ğ­±\n"
     ]
    }
   ],
   "source": [
    "chars=(set([*train_txt]))\n",
    "print(''.join(sorted(chars)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=txt2word(train_txt)\n",
    "unigram_words=generate_N_grams(train_words,ngram=1)\n",
    "uni_counts=create_Ngram_counts(unigram_words)\n",
    "uni_smooth=laplace_smoothing_counts(uni_counts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(set(unigram_words))\n",
    "# vocab.remove('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 0.0005628573403509465)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_counts['Ø³Ø§Ø®Øª'],uni_smooth['Ø³Ø§Ø®Øª']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.671880737313652e-05"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 1/(sum(uni_smooth.values()) + len(uni_smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998.5418528868581"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(test_words, uni_counts, vocab_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence, uni_smooth, vocab_size):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        uni_probs = [uni_smooth.get(word, 0.0) for word in vocab]\n",
    "        pred = vocab[uni_probs.index(max(uni_probs))]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Ùˆ\n",
      "Ø³Ø§Ø±ØªØ± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±ÙˆØ´Ù† ÙÚ©Ø±ÛŒ ÙØ¹Ø§Ù„ Ø§Ø² Ù†Ø¸Ø± Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø§ÛŒÙ† ØªÛŒÙ… Ø¯Ø± Ø³Ø§Ù„ Û±Û¸Û¹Û¹ Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø´Ø¯ Ùˆ ØªØ§ Ø¨Ù‡ Ø­Ø§Ù„ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ú©Ù„ÛŒØ³Ø§ÛŒ Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ú©Ø±ÛŒÙ… Ø®Ø§Ù† Ù¾Ø³ Ø§Ø² Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø¨Ø± Ø¯Ø´Ù…Ù†Ø§Ù† Ø®ÙˆØ¯ Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø¹Ù…ÙˆÙ…ÛŒ ØªØ±ÛŒÙ† Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø¬Ø§Ù†ÙˆØ±Ø§Ù† Ú©Ù‡ Ø·ÛŒ Ù…Ø±Ø§Ø­Ù„ Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ù†ÛŒØ² Ù¾ÛŒØ´ Ø§Ø² Ù‡Ù…Ù‡ Ùˆ\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence, uni_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Ùˆ\n",
      "Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† Ø§Ù†Ù‚Ù„Ø§Ø¨ Ù…Ø´Ø±ÙˆØ·Ù‡ Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ù†ÛŒØ±ÙˆÙ‡Ø§ÛŒ Ù…Ø­Ù…Ø¯Ø¹Ù„ÛŒ Ø´Ø§Ù‡ Ø¨Ø§   Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø´Ø±Ú©Øª Ø®Ø¯Ù…Ø§Øª Ù…Ø§Ù„ÛŒ Ùˆ  Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø´Ø®Øµ Ù…ÙˆØ³ÛŒ ØªÙˆØ±Ø§Øª Ø±Ø§   Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ù†Ø§Ù… Ø§Ù…Ø§Ø²ÙˆÙ† Ø±Ø§ Ø§Ø² ÛŒÚ© Ù„ØºØª Ù†Ø§Ù…Ù‡   Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "ØªÛŒÙ… Ø³Ù¾Ø§Ù‡Ø§Ù† Ø§ØµÙÙ‡Ø§Ù† Ú©Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø´Ø¹Ø¨Ù‡  Ùˆ\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,uni_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=txt2word(train_txt)\n",
    "bigram_words=generate_N_grams(train_words,ngram=2)\n",
    "bi_counts=create_Ngram_counts(bigram_words)\n",
    "bi_smooth=laplace_smoothing_counts(bi_counts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005727334444238721, 231)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_smooth['Ø¨Ù‡ Ø§ÛŒÙ†'],bi_conts['Ø¨Ù‡ Ø§ÛŒÙ†']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105707.71662553419"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(test_words, bi_counts, vocab_size,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict next word using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence, bi_smooth, vocab_size):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return None\n",
    "    elif len(words) == 2:\n",
    "        bi = (words[0], words[1])\n",
    "        bi_probs = [bi_smooth.get(' '.join([bi[0], word]),0.0) for word in vocab]\n",
    "        pred = vocab[bi_probs.index(max(bi_probs))]\n",
    "    else:\n",
    "        bi = (words[-2], words[-1])\n",
    "        bi_probs = [bi_smooth.get(' '.join([bi[0], word]), 0.0) for word in vocab]\n",
    "        pred = vocab[bi_probs.index(max(bi_probs))]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Ø§Ù†\n",
      "Ø³Ø§Ø±ØªØ± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±ÙˆØ´Ù† ÙÚ©Ø±ÛŒ ÙØ¹Ø§Ù„ Ø§Ø² Ù†Ø¸Ø± Ø§Ù†\n",
      "_______________________\n",
      "predicted_word: Ù‡Ø§ÛŒ\n",
      "Ø§ÛŒÙ† ØªÛŒÙ… Ø¯Ø± Ø³Ø§Ù„ Û±Û¸Û¹Û¹ Ù‡Ø§ÛŒ\n",
      "_______________________\n",
      "predicted_word: Ø³Ø§Ù„\n",
      "Ø´Ø¯ Ùˆ ØªØ§ Ø¨Ù‡ Ø­Ø§Ù„ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† Ø³Ø§Ù„\n",
      "_______________________\n",
      "predicted_word: Ø´Ù‡Ø±\n",
      "Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ú©Ù„ÛŒØ³Ø§ÛŒ Ø´Ù‡Ø±\n",
      "_______________________\n",
      "predicted_word: Ø§Ø²\n",
      "Ú©Ø±ÛŒÙ… Ø®Ø§Ù† Ù¾Ø³ Ø§Ø² Ø§Ø²\n",
      "_______________________\n",
      "predicted_word: Ù…Ø­Ù…Ø¯Ø´Ø§Ù‡\n",
      "Ø¨Ø± Ø¯Ø´Ù…Ù†Ø§Ù† Ø®ÙˆØ¯ Ù…Ø­Ù…Ø¯Ø´Ø§Ù‡\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø¹Ù…ÙˆÙ…ÛŒ ØªØ±ÛŒÙ† Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ø§Ù†\n",
      "Ø¬Ø§Ù†ÙˆØ±Ø§Ù† Ú©Ù‡ Ø·ÛŒ Ù…Ø±Ø§Ø­Ù„ Ø§Ù†\n",
      "_______________________\n",
      "predicted_word: Ø§Ù†\n",
      "Ù†ÛŒØ² Ù¾ÛŒØ´ Ø§Ø² Ù‡Ù…Ù‡ Ø§Ù†\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence, bi_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Ø¹Ø¨Ø§Ø³\n",
      "Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† Ø§Ù†Ù‚Ù„Ø§Ø¨ Ù…Ø´Ø±ÙˆØ·Ù‡ Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ù†ÛŒØ±ÙˆÙ‡Ø§ÛŒ Ù…Ø­Ù…Ø¯Ø¹Ù„ÛŒ Ø´Ø§Ù‡ Ø¨Ø§   Ø¹Ø¨Ø§Ø³\n",
      "_______________________\n",
      "predicted_word: Ùˆ\n",
      "Ø´Ø±Ú©Øª Ø®Ø¯Ù…Ø§Øª Ù…Ø§Ù„ÛŒ Ùˆ  Ùˆ\n",
      "_______________________\n",
      "predicted_word: Ø¨Ù‡\n",
      "Ø´Ø®Øµ Ù…ÙˆØ³ÛŒ ØªÙˆØ±Ø§Øª Ø±Ø§   Ø¨Ù‡\n",
      "_______________________\n",
      "predicted_word: Ø§ÛŒØªØ§Ù„ÛŒØ§ÛŒÛŒ\n",
      "Ù†Ø§Ù… Ø§Ù…Ø§Ø²ÙˆÙ† Ø±Ø§ Ø§Ø² ÛŒÚ© Ù„ØºØª Ù†Ø§Ù…Ù‡   Ø§ÛŒØªØ§Ù„ÛŒØ§ÛŒÛŒ\n",
      "_______________________\n",
      "predicted_word: Ø¨Ù‡\n",
      "ØªÛŒÙ… Ø³Ù¾Ø§Ù‡Ø§Ù† Ø§ØµÙÙ‡Ø§Ù† Ú©Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø´Ø¹Ø¨Ù‡  Ø¨Ù‡\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,bi_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigram_words=generate_N_grams(train_words,ngram=3)\n",
    "tri_counts=create_Ngram_counts(trigram_words)\n",
    "tri_smooth=laplace_smoothing_counts(tri_counts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.937369468294682e-06, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_smooth['Ø´Ù‡Ø± Ø¨Ù‡ Ø§ÛŒÙ†'],tri_counts['Ø´Ù‡Ø± Ø¨Ù‡ Ø§ÛŒÙ†']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict masked word trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence, tri_smooth, vocab_size):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        # print('small')\n",
    "        return None\n",
    "    elif len(words) == 2:\n",
    "        bi = (words[0], words[1])\n",
    "        tri_probs = [tri_smooth.get(' '.join([bi[0], bi[1], word]),0.0) for word in vocab]\n",
    "        pred= vocab[tri_probs.index(max(tri_probs))]\n",
    "    else:\n",
    "        tri = (words[-3], words[-2], words[-1])\n",
    "        bi = (words[-2], words[-1])\n",
    "        tri_probs = [tri_smooth.get(' '.join([tri[0], tri[1], word]),0.0) for word in vocab]\n",
    "        bi_probs = [tri_smooth.get(' '.join([bi[0], bi[1], word]), 0.0) for word in vocab]\n",
    "        probs = [p_tri * p_bi for p_tri, p_bi in zip(tri_probs, bi_probs)]\n",
    "        pred= vocab[probs.index(max(probs))]\n",
    "    # if pred=='':\n",
    "    #     idx=np.array(probs).argsort()[-2]\n",
    "    #     pred=vocab[idx]\n",
    "        # print(max(probs))\n",
    "\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "Ø³Ø§Ø±ØªØ± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±ÙˆØ´Ù† ÙÚ©Ø±ÛŒ ÙØ¹Ø§Ù„ Ø§Ø² Ù†Ø¸Ø± \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø§ÛŒÙ† ØªÛŒÙ… Ø¯Ø± Ø³Ø§Ù„ Û±Û¸Û¹Û¹ \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø´Ø¯ Ùˆ ØªØ§ Ø¨Ù‡ Ø­Ø§Ù„ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ú©Ù„ÛŒØ³Ø§ÛŒ \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ú©Ø±ÛŒÙ… Ø®Ø§Ù† Ù¾Ø³ Ø§Ø² \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø¨Ø± Ø¯Ø´Ù…Ù†Ø§Ù† Ø®ÙˆØ¯ \n",
      "_______________________\n",
      "predicted_word: Ù†Ø´Ø§Ù†\n",
      "Ø¹Ù…ÙˆÙ…ÛŒ ØªØ±ÛŒÙ† Ù†Ø´Ø§Ù†\n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø¬Ø§Ù†ÙˆØ±Ø§Ù† Ú©Ù‡ Ø·ÛŒ Ù…Ø±Ø§Ø­Ù„ \n",
      "_______________________\n",
      "predicted_word: Ø§ÛŒÙ†\n",
      "Ù†ÛŒØ² Ù¾ÛŒØ´ Ø§Ø² Ù‡Ù…Ù‡ Ø§ÛŒÙ†\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence,tri_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict next word trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† Ø§Ù†Ù‚Ù„Ø§Ø¨ Ù…Ø´Ø±ÙˆØ·Ù‡ Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ù†ÛŒØ±ÙˆÙ‡Ø§ÛŒ Ù…Ø­Ù…Ø¯Ø¹Ù„ÛŒ Ø´Ø§Ù‡ Ø¨Ø§   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø´Ø±Ú©Øª Ø®Ø¯Ù…Ø§Øª Ù…Ø§Ù„ÛŒ Ùˆ  \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ø´Ø®Øµ Ù…ÙˆØ³ÛŒ ØªÙˆØ±Ø§Øª Ø±Ø§   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "Ù†Ø§Ù… Ø§Ù…Ø§Ø²ÙˆÙ† Ø±Ø§ Ø§Ø² ÛŒÚ© Ù„ØºØª Ù†Ø§Ù…Ù‡   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "ØªÛŒÙ… Ø³Ù¾Ø§Ù‡Ø§Ù† Ø§ØµÙÙ‡Ø§Ù† Ú©Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø´Ø¹Ø¨Ù‡  \n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,tri_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 383253.2208318584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "perplexity=get_perplexity(test_words,tri_counts,vocab_size,3)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
