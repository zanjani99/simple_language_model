{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_path='NLP-01-2-HW1-Data/valid.txt'\n",
    "test_file_path='NLP-01-2-HW1-Data/test.txt'\n",
    "\n",
    "mask_file_path='NLP-01-2-HW1-Data/mask.txt'\n",
    "mask_gold_file_path='NLP-01-2-HW1-Data/mask_gold.txt'\n",
    "\n",
    "incomplete_file_path='NLP-01-2-HW1-Data/incomplete.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read  and preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    # load the dataset and return the txt as string\n",
    "    with open(path, 'r') as file:\n",
    "        txt=file.read()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(txt):\n",
    "    '''Preprocess the text by removing any unwanted characters,\n",
    "     converting all the text to lowercase'''\n",
    "\n",
    "\n",
    "    # remove any unwanted characters\n",
    "    txt=txt.replace('\\u200c',' ')\n",
    "    txt=txt.replace('\\n',' ')\n",
    "\n",
    "    txt= re.sub(r'[^\\w\\s]','',txt)\n",
    "    # convert all the english text to lowercase\n",
    "    txt=txt.lower()\n",
    "    \n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2word(txt):\n",
    "    # split the text into individual words.\n",
    "    words= txt.split(' ')\n",
    "    # remove '_' in case it appears individually as a word\n",
    "    if \"_\" in words:\n",
    "        words.remove(\"_\")\n",
    "    # delete empty strings\n",
    "    if \"\" in words:\n",
    "        words.remove(\"\")\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2sentence(txt):\n",
    "    # split the text into individual sentence based on punctuation\n",
    "    sentences = re.split(';|\\.|\\n|\\?|\\؟',txt)\n",
    "    # delete empty strings\n",
    "    if '' in sentences:\n",
    "        sentences.remove('')\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' سلام', ' بچه', 'حالت خوبه', 'چه خبر ', 'سالمی']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2sentence(' سلام? بچه.حالت خوبه;چه خبر ؟سالمی')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N grams functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(words,ngram=2):\n",
    "    # get list of words and create list of ngrams.\n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The above',\n",
       " 'above function',\n",
       " 'function inputs',\n",
       " 'inputs two',\n",
       " 'two parameters']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=txt2word('The above function inputs two parameters')\n",
    "generate_N_grams(words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laplce smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Ngram_counts(words):\n",
    "    # Creating N_gram dictionary {word: count}\n",
    "    Ngram_counts={}\n",
    "    for word in words:\n",
    "        if word not in Ngram_counts:\n",
    "            Ngram_counts[word]=0\n",
    "        Ngram_counts[word] += 1\n",
    "        \n",
    "    return Ngram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing_counts(Ngram_counts,V,k=1):\n",
    "    # Normalizing counts in Ngram dictionary to get probabilities and add lablce smoothing\n",
    "    '''k is smoothing parameter \n",
    "    V is the vocabulary size\n",
    "    (when k==1 it's laplace smoothing \n",
    "    otherwise it's add-k smoothing)'''\n",
    "     \n",
    "    smooth_Ngram=Ngram_counts.copy()\n",
    "    total_grams=sum(Ngram_counts.values()) # \n",
    "    for word,value in Ngram_counts.items():\n",
    "        if word not in Ngram_counts:\n",
    "            smooth_Ngram[word]=k/(total_grams+(k*V))\n",
    "        else:\n",
    "            smooth_Ngram[word]=(value+k)/(total_grams+(k*V))\n",
    "    return smooth_Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(trigram,tri_counts,V,k=1):\n",
    "    \"\"\"\n",
    "    calculate add one smoothing for single Ngram\n",
    "    if Ngram exist in dictionary --> retrun normalized count\n",
    "    if Ngram does not exist --> return constant(k/ncount of all n-grams in courpus+(k*V)) \n",
    "    trigram: string of N words\n",
    "        tri_count: N_gram dictionary\n",
    "        V: length of vocab\n",
    "    \"\"\"\n",
    "    total_grams=sum(tri_counts.values())\n",
    "    if trigram  not in tri_counts:\n",
    "        P_laplace=k/(total_grams+(k*V))\n",
    "    else:\n",
    "        P_laplace=(tri_counts[trigram]+k)/(total_grams+(k*V))\n",
    "    return P_laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence,n_counts,V,n):\n",
    "    # calculate Laplace smoothing and score of sentence\n",
    "    score=0.0\n",
    "    \n",
    "    for i in range(n-1, len(sentence)):\n",
    "        ngram=' '.join(sentence[i-(n-1):i+1])\n",
    "        \n",
    "        p_laplace=laplace_smoothing(ngram,n_counts,V)\n",
    "        \n",
    "\n",
    "        score += math.log( p_laplace, 2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(corpus,n_counts,vocab_size,n):\n",
    "    # calculate the perplexity using score of each sentence\n",
    "    logprob=0\n",
    "    word_count=0\n",
    "    for sentence in corpus:\n",
    "        logprob += score_sentence(sentence,n_counts,vocab_size,n)\n",
    "        word_count += len(sentence) - (n-1)\n",
    "    logprob /= word_count\n",
    "    perplexity=2**(-logprob)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# laod and process train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess train data\n",
    "train_txt=load_dataset(\"NLP-01-2-HW1-Data/train.txt\")\n",
    "train_txt=preprocess_txt(train_txt)\n",
    "train_words=txt2word(train_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_courpus=load_dataset(\"NLP-01-2-HW1-Data/test.txt\")\n",
    "preprocess_test_courpus=[preprocess_txt(sentence) for sentence in txt2sentence(test_courpus)]\n",
    "test_words=[txt2word(sentence) for sentence in preprocess_test_courpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load masked and incomplete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sentence=[]\n",
    "with open(mask_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "       \n",
    "        words=(re.split('\\u200c| ',(line.split('\\t')[1])))\n",
    "        for j in range(words.count('#')):\n",
    "            if j==0:\n",
    "                idx=0\n",
    "                pre_idx=0\n",
    "            else:\n",
    "                pre_idx=idx+1\n",
    "            idx=words.index('#',idx+1)\n",
    "            # print(words[pre_idx:idx],j)\n",
    "            mask_sentence.append(' '.join(words[pre_idx:idx]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_data=[]\n",
    "with open(incomplete_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "        words=(re.split('\\u200c| ',preprocess_txt(line.split('\\t')[1])))\n",
    "        incomplete_data.append(' '.join(words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(set(train_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print charchters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0123456789_abcdefghijklmnopqrstuvwxyzæðıɑɒɔəɛɨɪɾʁʃʊʏˈːαβγδεηθικλμνορςστυωабгдеиклнопрстучшъяіաբեէթիլխկհճմյնոչպստրւքօאבדהוחיכלמןנערשתءآأئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىي١٢٥٧٨٩ٱپچژکگڵہۆیێۏە۰۱۲۳۴۵۶۷۸۹ईएखगदबमयरलशसকঠথদনবরசனᄀᄃᄋᄌᄎ하ᅢᅧᅩᅵᆫᆼⴰⴽⵎⵓⵔⵛ三中丹产佛党共勝北吉同唐国國土子字学官家振敏文方普朝李柴榴汉漢犬狮猪獅琵琶產田石秀羽胡臣船芸菩萄萨葡薩藩蜜術話語话语豊豬通郎酪長院馬马鵝鹅黨𐬀𐬎𐬙𐬚𐬭𐬰𐬱𐭥𐭧𐭩𐭯𐭱\n"
     ]
    }
   ],
   "source": [
    "chars=(set([*train_txt]))\n",
    "print(''.join(sorted(chars)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=txt2word(train_txt)\n",
    "unigram_words=generate_N_grams(train_words,ngram=1)\n",
    "uni_counts=create_Ngram_counts(unigram_words)\n",
    "uni_smooth=laplace_smoothing_counts(uni_counts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(set(unigram_words))\n",
    "# vocab.remove('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 0.0005628573403509465)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_counts['ساخت'],uni_smooth['ساخت']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.671880737313652e-05"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 1/(sum(uni_smooth.values()) + len(uni_smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998.5418528868581"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(test_words, uni_counts, vocab_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence, uni_smooth, vocab_size):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        uni_probs = [uni_smooth.get(word, 0.0) for word in vocab]\n",
    "        pred = vocab[uni_probs.index(max(uni_probs))]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: و\n",
      "سارتر به عنوان روشن فکری فعال از نظر و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "این تیم در سال ۱۸۹۹ و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "شد و تا به حال موفق به کسب یک عنوان و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "بزرگترین کلیسای و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "کریم خان پس از و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "بر دشمنان خود و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "عمومی ترین و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "جانوران که طی مراحل و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "نیز پیش از همه و\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence, uni_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: و\n",
      "در جریان انقلاب مشروطه ابتدا به عنوان یکی از نیروهای محمدعلی شاه با   و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "شرکت خدمات مالی و  و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "شخص موسی تورات را   و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "نام امازون را از یک لغت نامه   و\n",
      "_______________________\n",
      "predicted_word: و\n",
      "تیم سپاهان اصفهان که در ابتدا شعبه  و\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,uni_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=txt2word(train_txt)\n",
    "bigram_words=generate_N_grams(train_words,ngram=2)\n",
    "bi_counts=create_Ngram_counts(bigram_words)\n",
    "bi_smooth=laplace_smoothing_counts(bi_counts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005727334444238721, 231)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_smooth['به این'],bi_conts['به این']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105707.71662553419"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(test_words, bi_counts, vocab_size,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict next word using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence, bi_smooth, vocab_size):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return None\n",
    "    elif len(words) == 2:\n",
    "        bi = (words[0], words[1])\n",
    "        bi_probs = [bi_smooth.get(' '.join([bi[0], word]),0.0) for word in vocab]\n",
    "        pred = vocab[bi_probs.index(max(bi_probs))]\n",
    "    else:\n",
    "        bi = (words[-2], words[-1])\n",
    "        bi_probs = [bi_smooth.get(' '.join([bi[0], word]), 0.0) for word in vocab]\n",
    "        pred = vocab[bi_probs.index(max(bi_probs))]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: ان\n",
      "سارتر به عنوان روشن فکری فعال از نظر ان\n",
      "_______________________\n",
      "predicted_word: های\n",
      "این تیم در سال ۱۸۹۹ های\n",
      "_______________________\n",
      "predicted_word: سال\n",
      "شد و تا به حال موفق به کسب یک عنوان سال\n",
      "_______________________\n",
      "predicted_word: شهر\n",
      "بزرگترین کلیسای شهر\n",
      "_______________________\n",
      "predicted_word: از\n",
      "کریم خان پس از از\n",
      "_______________________\n",
      "predicted_word: محمدشاه\n",
      "بر دشمنان خود محمدشاه\n",
      "_______________________\n",
      "predicted_word: و\n",
      "عمومی ترین و\n",
      "_______________________\n",
      "predicted_word: ان\n",
      "جانوران که طی مراحل ان\n",
      "_______________________\n",
      "predicted_word: ان\n",
      "نیز پیش از همه ان\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence, bi_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: عباس\n",
      "در جریان انقلاب مشروطه ابتدا به عنوان یکی از نیروهای محمدعلی شاه با   عباس\n",
      "_______________________\n",
      "predicted_word: و\n",
      "شرکت خدمات مالی و  و\n",
      "_______________________\n",
      "predicted_word: به\n",
      "شخص موسی تورات را   به\n",
      "_______________________\n",
      "predicted_word: ایتالیایی\n",
      "نام امازون را از یک لغت نامه   ایتالیایی\n",
      "_______________________\n",
      "predicted_word: به\n",
      "تیم سپاهان اصفهان که در ابتدا شعبه  به\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,bi_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigram_words=generate_N_grams(train_words,ngram=3)\n",
    "tri_counts=create_Ngram_counts(trigram_words)\n",
    "tri_smooth=laplace_smoothing_counts(tri_counts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.937369468294682e-06, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_smooth['شهر به این'],tri_counts['شهر به این']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict masked word trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence, tri_smooth, vocab_size):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        # print('small')\n",
    "        return None\n",
    "    elif len(words) == 2:\n",
    "        bi = (words[0], words[1])\n",
    "        tri_probs = [tri_smooth.get(' '.join([bi[0], bi[1], word]),0.0) for word in vocab]\n",
    "        pred= vocab[tri_probs.index(max(tri_probs))]\n",
    "    else:\n",
    "        tri = (words[-3], words[-2], words[-1])\n",
    "        bi = (words[-2], words[-1])\n",
    "        tri_probs = [tri_smooth.get(' '.join([tri[0], tri[1], word]),0.0) for word in vocab]\n",
    "        bi_probs = [tri_smooth.get(' '.join([bi[0], bi[1], word]), 0.0) for word in vocab]\n",
    "        probs = [p_tri * p_bi for p_tri, p_bi in zip(tri_probs, bi_probs)]\n",
    "        pred= vocab[probs.index(max(probs))]\n",
    "    # if pred=='':\n",
    "    #     idx=np.array(probs).argsort()[-2]\n",
    "    #     pred=vocab[idx]\n",
    "        # print(max(probs))\n",
    "\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "سارتر به عنوان روشن فکری فعال از نظر \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "این تیم در سال ۱۸۹۹ \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "شد و تا به حال موفق به کسب یک عنوان \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "بزرگترین کلیسای \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "کریم خان پس از \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "بر دشمنان خود \n",
      "_______________________\n",
      "predicted_word: نشان\n",
      "عمومی ترین نشان\n",
      "_______________________\n",
      "predicted_word: Null\n",
      "جانوران که طی مراحل \n",
      "_______________________\n",
      "predicted_word: این\n",
      "نیز پیش از همه این\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in mask_sentence:\n",
    "    predicted_word = predict_next_word(sentence,tri_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict next word trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Null\n",
      "در جریان انقلاب مشروطه ابتدا به عنوان یکی از نیروهای محمدعلی شاه با   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "شرکت خدمات مالی و  \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "شخص موسی تورات را   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "نام امازون را از یک لغت نامه   \n",
      "_______________________\n",
      "predicted_word: Null\n",
      "تیم سپاهان اصفهان که در ابتدا شعبه  \n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "for sentence in incomplete_data:\n",
    "    predicted_word = predict_next_word(sentence,tri_smooth, vocab_size)\n",
    "    if predicted_word=='':\n",
    "        print('predicted_word:','Null')\n",
    "    else:\n",
    "        print('predicted_word:',predicted_word)\n",
    "    print(' '.join([sentence,predicted_word]))\n",
    "    print(\"_______________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 383253.2208318584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "perplexity=get_perplexity(test_words,tri_counts,vocab_size,3)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
