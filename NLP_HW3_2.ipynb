{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import scipy\n",
    "import collections\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from torcheval.metrics.text import Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO delete words with less than 5 times repete from vocab @done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_path='NLP-01-2-HW1-Data/valid.txt'\n",
    "mask_file_path='NLP-01-2-HW1-Data/mask.txt'\n",
    "mask_gold_file_path='NLP-01-2-HW1-Data/mask_gold.txt'\n",
    "\n",
    "incomplete_file_path='NLP-01-2-HW1-Data/incomplete.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP-01-2-HW1-Data/train.txt\n"
     ]
    }
   ],
   "source": [
    "# mode='coding'\n",
    "mode='training'\n",
    "if mode =='training':\n",
    "    corpus_file_path = 'NLP-01-2-HW1-Data/train.txt'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    num_epochs = 20\n",
    "else:\n",
    "    corpus_file_path = 'NLP-01-2-HW1-Data/test.txt'\n",
    "\n",
    "    # device = 'cpu'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    num_epochs = 2\n",
    "\n",
    "print(corpus_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using cuda with 16 cpus\n"
     ]
    }
   ],
   "source": [
    "available_workers = multiprocessing.cpu_count()\n",
    "print(f' using {device} with {available_workers} cpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path, 'r') as file:\n",
    "        txt=file.read()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(txt):\n",
    "    '''Preprocess the text by removing any unwanted characters,\n",
    "     converting all the text to lowercase'''\n",
    "    # txt=\" \".join(re.findall(r\"[\\u0600-\\u06FF]+\", txt)) # delete all non_ persion texts\n",
    "\n",
    "    # remove any unwanted characters\n",
    "    # txt=txt.replace('\\u200c',' ')\n",
    "    txt=txt.replace('\\n',' ')\n",
    "    # txt = re.sub(\"\\d+\" ,\" \", txt)# delete numbers\n",
    "\n",
    "    # txt= re.sub(r'[^\\w\\s]','',txt)\n",
    "    # convert all the english text to lowercase\n",
    "    txt=re.sub(' +', ' ',txt)\n",
    "    # txt=txt.lower()\n",
    "    \n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'همچنین جمالت فایلهای txt.mask . ۶۷و hjg txt.incomplete '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_txt('همچنین جمالت فایلهای txt.mask . ۶۷و  hjg txt.incomplete ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2word(txt):\n",
    "    # split the text into individual words.\n",
    "    words= txt.split(' ')\n",
    "    # remove '_' in case it appears individually as a word\n",
    "    if \"_\" in words:\n",
    "        words.remove(\"_\")\n",
    "    # delete empty strings\n",
    "    if \"\" in words:\n",
    "        words.remove(\"\")\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2sentence(txt):\n",
    "    # split the text into individual sentence.\n",
    "    sentences=re.split(';|\\.|\\n|\\?|\\؟',txt)\n",
    "    # delete empty strings\n",
    "    if '' in sentences:\n",
    "        sentences.remove('')\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(corpus_txt):\n",
    "    # create voacab of corpus and delete the low frequent word from it\n",
    "    words_term_frequency_train={}\n",
    "    # UNK_symbol = \"<UNK>\"\n",
    "    vocab=set()\n",
    "    words_list = txt2word(corpus_txt)\n",
    "    # create term frequency of the words\n",
    "    for word in words_list:\n",
    "        words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "    # create vocabulary and delete low frequency words\n",
    "    for word in words_list:\n",
    "        if words_term_frequency_train.get(word,0) >= 5:\n",
    "            vocab.add(word)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, corpus_file_path, vocab,n=2):\n",
    "        \n",
    "        self.n=n\n",
    "        self.corpus =self.load_dataset(corpus_file_path)\n",
    "        self.corpus=[self.preprocess_txt(sentence) for sentence in self.txt2sentence(self.corpus)]\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size=len(vocab)+1\n",
    "        self.clean_corpus()\n",
    "        self.word_to_id_mappings=self.create_mapper()\n",
    "        self.X,self.y=self.create_dataset()\n",
    "\n",
    "\n",
    "\n",
    "    def create_mapper(self):\n",
    "        word_to_id_mappings={'<UNK>':0}\n",
    "        for idx, word in enumerate(self.vocab,1):\n",
    "            word_to_id_mappings[word]=idx\n",
    "        return word_to_id_mappings\n",
    "\n",
    "    def word2idx(self,word):\n",
    "        unknown_word_id = self.word_to_id_mappings['<UNK>']\n",
    "        return self.word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "    def load_dataset(self,path):\n",
    "        with open(path, 'r') as file:\n",
    "            txt=file.read()\n",
    "        return txt\n",
    "\n",
    "    def preprocess_txt(self,txt):\n",
    "        '''Preprocess the text by removing any unwanted characters,\n",
    "        converting all the text to lowercase'''\n",
    "        # delete all non_ persion texts\n",
    "        txt=\" \".join(re.findall(r\"[\\u0600-\\u06FF]+\", txt)) \n",
    "        # remove any unwanted characters\n",
    "        txt=txt.replace('\\u200c',' ')\n",
    "        txt=txt.replace('\\n',' ')\n",
    "        txt = re.sub(\"\\d+\" ,\" \", txt)# delete numbers\n",
    "        txt= re.sub(r'[^\\w\\s]','',txt)\n",
    "        # convert all the english text to lowercase\n",
    "        txt=re.sub(' +', ' ',txt)\n",
    "        # convert all the english text to lowercase\n",
    "        txt=txt.lower()  \n",
    "        return txt\n",
    "\n",
    "    def txt2sentence(self,txt):\n",
    "        # split the text into individual sentence.\n",
    "        sentences=re.split('|;|\\.|\\n|\\?|\\؟',txt)\n",
    "        # delete empty strings\n",
    "        sentences=list (filter(lambda sent : sent!= '', sentences))\n",
    "        \n",
    "        return sentences\n",
    "        \n",
    "    \n",
    "    def clean_corpus(self):\n",
    "        self.corpus=list (filter(lambda sentence : sentence!= '', self.corpus))\n",
    "\n",
    "\n",
    "    def txt2word(self,txt):\n",
    "        # split the text into individual words.\n",
    "        words= txt.split(' ')\n",
    "        # remove '_' in case it appears individually as a word\n",
    "        if \"_\" in words:\n",
    "            words.remove(\"_\")\n",
    "        # delete empty strings\n",
    "        if \"\" in words:\n",
    "            words.remove(\"\")\n",
    "            \n",
    "        return words\n",
    "    \n",
    "\n",
    "    def create_dataset(self):\n",
    "        X=[]\n",
    "        y=[]\n",
    "        for sentence in self.corpus:\n",
    "            words_list=self.txt2word(sentence)\n",
    "            for i in range(len(words_list)):\n",
    "                if len(words_list)<= i+self.n:\n",
    "                    # ignoring sentece less than contex size \n",
    "                    # (if n=3 less than 4 words )\n",
    "                    break\n",
    "                \n",
    "                x_extract=[self.word2idx(words_list[i+j]) for j in range(self.n) ]\n",
    "                y_extraxt=[self.word2idx(words_list[i+self.n])]\n",
    "                # for n=3 x is[i,i+1,i+2]\n",
    "                # and y is [i+3]\n",
    "                \n",
    "                X.append(x_extract)\n",
    "                y.append(y_extraxt)\n",
    "        \n",
    "        return np.array(X),np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x_1 = torch.from_numpy(self.X[idx]).to(device)\n",
    "        y_1 = torch.tensor(self.y[idx]).to(device)\n",
    "        return x_1,y_1\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus=load_dataset(corpus_file_path)\n",
    "# train_corpus=preprocess_txt(train_corpus)\n",
    "# create_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNNLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_dim,n=2):\n",
    "        ''' output_dim=vocab_size\n",
    "        n : N previous word that we want to predict based on them (input context_size)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding=nn.Embedding(vocab_size, embedding_dim)# embeding layer E(V*d)\n",
    "        self.hidden_layer=nn.Linear(n*embedding_dim, hidden_dim)# hidden layer W(nd*d_h)\n",
    "        self.output_layer=nn.Linear(hidden_dim, vocab_size,bias = False)# output layer U (d_h*V)\n",
    "        self.n=n #input context_size\n",
    "    def forward(self,x):\n",
    " \n",
    "        # compute X : v*nd  if n=3 it will concatinate X1,X2,X3 for trigram model\n",
    "        embedded = self.embedding(x).view((-1,self.n * self.embedding_dim))\n",
    "        # compute  hidden layer\n",
    "        hidden = F.relu(self.hidden_layer(embedded))\n",
    "        # compute softmax of output\n",
    "        output = F.softmax(self.output_layer(hidden), dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_charts(train_losses,train_accu):\n",
    "    # plot accuracy and loss chart\n",
    "    fig = plt.figure()\n",
    "    plt.plot(list(range(num_epochs)), train_losses, color='blue')\n",
    "    plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('negative log likelihood loss')\n",
    "    plt.savefig(\"loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(list(range(num_epochs)), train_accu, color='blue')\n",
    "    plt.legend(['Train accuracy'], loc='upper right')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.savefig(\"loss.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(data_loader,model,criterion,optimizer,num_epochs):\n",
    "    train_losses = []\n",
    "    train_accu=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        st = time.time()\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            acc = get_accuracy_from_log_probs(outputs, labels)\n",
    "            \n",
    "            loss = criterion(outputs,labels.squeeze())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 0: \n",
    "                print(\"Training Iteration {} of epoch {}  Loss: {}; Acc:{}; Time taken (s): {}\".format(\n",
    "                    i, epoch, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "        train_losses.append(running_loss/len(data_loader))\n",
    "        train_accu.append((100*acc.cpu().numpy()))\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/i:.4f},acc: {acc}')\n",
    "    return model,train_losses,train_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trigram NN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size=3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus=load_dataset(corpus_file_path)\n",
    "train_preprocess_corpus=preprocess_txt(train_corpus)\n",
    "# train_vocab=create_vocab(train_corpus)\n",
    "vocab= create_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus=load_dataset(val_file_path)\n",
    "val_preprocess_corpus=preprocess_txt(train_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200#len(train_dense_embed)\n",
    "hidden_dim = 100\n",
    "vocab_size = len(vocab)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# laod data and create trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus=load_dataset(corpus_file_path)\n",
    "train_preprocess_corpus=preprocess_txt(train_corpus)\n",
    "# train_vocab=create_vocab(train_corpus)\n",
    "vocab= create_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_dataset = CorpusDataset(mask_file_path, vocab, n=context_size)\n",
    "# mask_data_loader = DataLoader(mask_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapper(vocab):\n",
    "    word_to_id_mappings={'<UNK>':0}\n",
    "    for idx, word in enumerate(vocab,1):\n",
    "        word_to_id_mappings[word]=idx\n",
    "    return word_to_id_mappings\n",
    "\n",
    "def word2idx(word,word_to_id_mappings):\n",
    "    unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "    return int(word_to_id_mappings.get(word,unknown_word_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(id,vocab):\n",
    "    if id==0:\n",
    "        return '<UNK>'\n",
    "    else:\n",
    "        return list(vocab)[id+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id_mappings=create_mapper(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data=[]\n",
    "with open(mask_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "        words=(re.split('\\u200c| ',preprocess_txt(line.split('\\t')[1])))\n",
    "        for j in range(words.count('#')):\n",
    "            if j==0:\n",
    "                idx=0\n",
    "            idx=words.index('#',idx+1)\n",
    "            if idx <=3:\n",
    "                trigram=['']+words[idx-2:idx]\n",
    "                # print(trigram)\n",
    "            else:\n",
    "                trigram=(words[idx-3:idx])\n",
    "            x_extract=np.array([[word2idx(trigram[i],word_to_id_mappings) \\\n",
    "                for i in range(3)]],dtype=np.int64)\n",
    "            x_extract=torch.from_numpy(x_extract).to(device)\n",
    "            mask_data.append([x_extract,trigram])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_extract=[word2idx(tri_model[i],word_to_id_mappings) for  in range(3)]\n",
    "#             print(x_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model=torch.load('./FFLM_20_3gram.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['فعال', 'از', 'نظر']\n",
      "<UNK>\n",
      "['در', 'سال', '۱۸۹۹']\n",
      "<UNK>\n",
      "['کسب', 'یک', 'عنوان']\n",
      "<UNK>\n",
      "['', 'بزرگترین', 'کلیسای']\n",
      "<UNK>\n",
      "['خان', 'پس', 'از']\n",
      "<UNK>\n",
      "['بر', 'دشمنان', 'خود']\n",
      "<UNK>\n",
      "['', 'عمومی', 'ترین']\n",
      "<UNK>\n",
      "['که', 'طی', 'مراحل']\n",
      "<UNK>\n",
      "['پیش', 'از', 'همه']\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "tri_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs ,trigram in mask_data:\n",
    "        print(trigram)\n",
    "        output = tri_model(inputs)\n",
    "        out=(output.cpu().numpy())\n",
    "        if np.argmax(out)==0 and np.max(out)!=1:\n",
    "            idx=out.argsort()[:,-2][0]\n",
    "        else:\n",
    "            idx=np.argmax(out)\n",
    "        # print(idx)\n",
    "        print(idx2word(idx,vocab))\n",
    "        # break\n",
    "        # print(inputs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_data=[]\n",
    "with open(incomplete_file_path) as file_in:\n",
    "    for line in file_in:\n",
    "        words=(re.split('\\u200c| ',preprocess_txt(line.split('\\t')[1])))\n",
    "        idx=len(words)\n",
    "        # if idx <=3:\n",
    "        #     trigram=['']+words[idx-2:idx]\n",
    "        #     # print(trigram)\n",
    "        # else:\n",
    "        trigram=(words[idx-4:idx-1])\n",
    "        x_extract=np.array([[word2idx(trigram[i],word_to_id_mappings) \\\n",
    "            for i in range(3)]],dtype=np.int64)\n",
    "        x_extract=torch.from_numpy(x_extract).to(device)\n",
    "        incomplete_data.append([x_extract,trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['محمدعلی', 'شاه', 'با']\n",
      "<UNK>\n",
      "['خدمات', 'مالی', 'و']\n",
      "<UNK>\n",
      "['موسی', 'تورات', 'را']\n",
      "<UNK>\n",
      "['یک', 'لغت', 'نامه']\n",
      "<UNK>\n",
      "['در', 'ابتدا', 'شعبه']\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "tri_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs ,trigram in incomplete_data:\n",
    "        print(trigram)\n",
    "        output = tri_model(inputs)\n",
    "        out=(output.cpu().numpy())\n",
    "        if np.argmax(out)==0 and np.max(out)!=1:\n",
    "            idx=out.argsort()[:,-2][0]\n",
    "        else:\n",
    "            idx=np.argmax(out)\n",
    "        # print(idx)\n",
    "        print(idx2word(idx,vocab))\n",
    "        # break\n",
    "        # print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
